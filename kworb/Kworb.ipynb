{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12df6f2c-d0d6-4ccb-8a8a-1c2695590f58",
   "metadata": {},
   "source": [
    "### Spotify Top 200 Artist_US Daily Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39c5e63-0cc2-44a9-a9c9-b63e80396a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "KWORB_DAILY_URL = \"https://kworb.net/spotify/country/us_daily.html\"\n",
    "BASE = \"https://kworb.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66211e5-010a-4d7a-976f-964ca78c8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_url(href):\n",
    "    \"\"\"\n",
    "    Normalize track or artist href from Kworb.\n",
    "    Ensures:\n",
    "    - no \"../\"\n",
    "    - includes /spotify/\n",
    "    - full absolute URL\n",
    "    \"\"\"\n",
    "    if not href:\n",
    "        return None\n",
    "\n",
    "    # convert relative ‚Üí absolute (removes ../)\n",
    "    url = urljoin(BASE + \"/\", href)\n",
    "\n",
    "    # auto fix missing /spotify/ for tracks\n",
    "    if \"/track/\" in url and \"/spotify/\" not in url:\n",
    "        url = url.replace(\"/track/\", \"/spotify/track/\")\n",
    "\n",
    "    # auto fix missing /spotify/ for artists\n",
    "    if \"/artist/\" in url and \"/spotify/\" not in url:\n",
    "        url = url.replace(\"/artist/\", \"/spotify/artist/\")\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c980674-b577-4d15-ab4e-e63461a17b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(td):\n",
    "    links = td.find_all(\"a\")\n",
    "    artist_url = None\n",
    "    track_url = None\n",
    "\n",
    "    for a in links:\n",
    "        raw = a.get(\"href\", \"\")\n",
    "        if not raw:\n",
    "            continue\n",
    "\n",
    "        norm = normalize_url(raw)\n",
    "\n",
    "        if \"/track/\" in norm:\n",
    "            track_url = norm\n",
    "        elif \"/artist/\" in norm:\n",
    "            artist_url = norm\n",
    "\n",
    "    return artist_url, track_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc3de56-376c-4f21-bbc0-9d544d522999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_songs_url_from_track(track_url):\n",
    "    try:\n",
    "        r = requests.get(track_url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        a = soup.find(\"a\", href=lambda x: x and \"/artist/\" in x)\n",
    "        if not a:\n",
    "            return None\n",
    "\n",
    "        href = a.get(\"href\", \"\")\n",
    "        norm = normalize_url(href)\n",
    "\n",
    "        artist_id = norm.split(\"/\")[-1].replace(\".html\", \"\")\n",
    "\n",
    "        final_url = f\"{BASE}/spotify/artist/{artist_id}_albums.html\"\n",
    "        return final_url\n",
    "\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3724cc6-318a-48c3-a3e6-2d496bf47fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>daily_streams</th>\n",
       "      <th>streams_plus</th>\n",
       "      <th>seven_day</th>\n",
       "      <th>seven_day_plus</th>\n",
       "      <th>total_streams</th>\n",
       "      <th>artist_url</th>\n",
       "      <th>track_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>The Fate of Ophelia</td>\n",
       "      <td>1,341,674</td>\n",
       "      <td>-56,235</td>\n",
       "      <td>10,048,135</td>\n",
       "      <td>-144,921</td>\n",
       "      <td>137,000,080</td>\n",
       "      <td>https://kworb.net/spotify/artist/06HL4z0CvFAxy...</td>\n",
       "      <td>https://kworb.net/spotify/track/53iuhJlwXhSER5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>HUNTR/X</td>\n",
       "      <td>Golden (w/ Ejae, AUDREY NUNA, REI AMI, KPop De...</td>\n",
       "      <td>1,289,935</td>\n",
       "      <td>+68,272</td>\n",
       "      <td>9,225,980</td>\n",
       "      <td>-31,950</td>\n",
       "      <td>249,189,200</td>\n",
       "      <td>https://kworb.net/spotify/artist/2yNNYQBChuox9...</td>\n",
       "      <td>https://kworb.net/spotify/track/1CPZ5BxNNd0n0n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Olivia Dean</td>\n",
       "      <td>Man I Need</td>\n",
       "      <td>1,261,178</td>\n",
       "      <td>+18,782</td>\n",
       "      <td>8,635,735</td>\n",
       "      <td>+120,751</td>\n",
       "      <td>88,428,560</td>\n",
       "      <td>https://kworb.net/spotify/artist/00x1fYSGhdqSc...</td>\n",
       "      <td>https://kworb.net/spotify/track/1qbmS6ep2hbBRa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Olivia Dean</td>\n",
       "      <td>So Easy (To Fall In Love)</td>\n",
       "      <td>954,986</td>\n",
       "      <td>+1,780</td>\n",
       "      <td>6,450,213</td>\n",
       "      <td>+127,544</td>\n",
       "      <td>36,909,384</td>\n",
       "      <td>https://kworb.net/spotify/artist/00x1fYSGhdqSc...</td>\n",
       "      <td>https://kworb.net/spotify/track/6sGIMrtIzQjdzN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Opalite</td>\n",
       "      <td>902,607</td>\n",
       "      <td>+15,398</td>\n",
       "      <td>6,797,306</td>\n",
       "      <td>-122,781</td>\n",
       "      <td>106,247,004</td>\n",
       "      <td>https://kworb.net/spotify/artist/06HL4z0CvFAxy...</td>\n",
       "      <td>https://kworb.net/spotify/track/3yWuTOYDztXjZx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Alex Warren</td>\n",
       "      <td>Ordinary</td>\n",
       "      <td>857,833</td>\n",
       "      <td>+84,007</td>\n",
       "      <td>5,755,462</td>\n",
       "      <td>+60,335</td>\n",
       "      <td>285,591,972</td>\n",
       "      <td>https://kworb.net/spotify/artist/0fTSzq9jAh4c3...</td>\n",
       "      <td>https://kworb.net/spotify/track/2RkZ5LkEzeHGRs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>sombr</td>\n",
       "      <td>back to friends</td>\n",
       "      <td>829,226</td>\n",
       "      <td>-42,387</td>\n",
       "      <td>5,942,307</td>\n",
       "      <td>-22,904</td>\n",
       "      <td>244,046,523</td>\n",
       "      <td>https://kworb.net/spotify/artist/4G9NDjRyZFDlJ...</td>\n",
       "      <td>https://kworb.net/spotify/track/0FTmksd2dxiE5e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>The Neighbourhood</td>\n",
       "      <td>Sweater Weather</td>\n",
       "      <td>723,460</td>\n",
       "      <td>-8,070</td>\n",
       "      <td>5,017,498</td>\n",
       "      <td>+31,029</td>\n",
       "      <td>976,280,232</td>\n",
       "      <td>https://kworb.net/spotify/artist/77SW9BnxLY8rJ...</td>\n",
       "      <td>https://kworb.net/spotify/track/2QjOHCTQ1Jl3za...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>The Goo Goo Dolls</td>\n",
       "      <td>Iris</td>\n",
       "      <td>702,524</td>\n",
       "      <td>+14,587</td>\n",
       "      <td>4,524,340</td>\n",
       "      <td>+58,388</td>\n",
       "      <td>297,272,926</td>\n",
       "      <td>https://kworb.net/spotify/artist/2sil8z5kiy4r7...</td>\n",
       "      <td>https://kworb.net/spotify/track/6Qyc6fS4DsZjB2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Brenda Lee</td>\n",
       "      <td>Rockin' Around The Christmas Tree</td>\n",
       "      <td>691,084</td>\n",
       "      <td>+8,144</td>\n",
       "      <td>4,697,247</td>\n",
       "      <td>-74,798</td>\n",
       "      <td>469,459,111</td>\n",
       "      <td>https://kworb.net/spotify/artist/4cPHsZM98sKzm...</td>\n",
       "      <td>https://kworb.net/spotify/track/2EjXfH91m7f8Hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rank        artist_name                                         track_name  \\\n",
       "0    1       Taylor Swift                                The Fate of Ophelia   \n",
       "1    2            HUNTR/X  Golden (w/ Ejae, AUDREY NUNA, REI AMI, KPop De...   \n",
       "2    3        Olivia Dean                                         Man I Need   \n",
       "3    4        Olivia Dean                          So Easy (To Fall In Love)   \n",
       "4    5       Taylor Swift                                            Opalite   \n",
       "5    6        Alex Warren                                           Ordinary   \n",
       "6    7              sombr                                    back to friends   \n",
       "7    8  The Neighbourhood                                    Sweater Weather   \n",
       "8    9  The Goo Goo Dolls                                               Iris   \n",
       "9   10         Brenda Lee                  Rockin' Around The Christmas Tree   \n",
       "\n",
       "  daily_streams streams_plus   seven_day seven_day_plus total_streams  \\\n",
       "0     1,341,674      -56,235  10,048,135       -144,921   137,000,080   \n",
       "1     1,289,935      +68,272   9,225,980        -31,950   249,189,200   \n",
       "2     1,261,178      +18,782   8,635,735       +120,751    88,428,560   \n",
       "3       954,986       +1,780   6,450,213       +127,544    36,909,384   \n",
       "4       902,607      +15,398   6,797,306       -122,781   106,247,004   \n",
       "5       857,833      +84,007   5,755,462        +60,335   285,591,972   \n",
       "6       829,226      -42,387   5,942,307        -22,904   244,046,523   \n",
       "7       723,460       -8,070   5,017,498        +31,029   976,280,232   \n",
       "8       702,524      +14,587   4,524,340        +58,388   297,272,926   \n",
       "9       691,084       +8,144   4,697,247        -74,798   469,459,111   \n",
       "\n",
       "                                          artist_url  \\\n",
       "0  https://kworb.net/spotify/artist/06HL4z0CvFAxy...   \n",
       "1  https://kworb.net/spotify/artist/2yNNYQBChuox9...   \n",
       "2  https://kworb.net/spotify/artist/00x1fYSGhdqSc...   \n",
       "3  https://kworb.net/spotify/artist/00x1fYSGhdqSc...   \n",
       "4  https://kworb.net/spotify/artist/06HL4z0CvFAxy...   \n",
       "5  https://kworb.net/spotify/artist/0fTSzq9jAh4c3...   \n",
       "6  https://kworb.net/spotify/artist/4G9NDjRyZFDlJ...   \n",
       "7  https://kworb.net/spotify/artist/77SW9BnxLY8rJ...   \n",
       "8  https://kworb.net/spotify/artist/2sil8z5kiy4r7...   \n",
       "9  https://kworb.net/spotify/artist/4cPHsZM98sKzm...   \n",
       "\n",
       "                                           track_url  \n",
       "0  https://kworb.net/spotify/track/53iuhJlwXhSER5...  \n",
       "1  https://kworb.net/spotify/track/1CPZ5BxNNd0n0n...  \n",
       "2  https://kworb.net/spotify/track/1qbmS6ep2hbBRa...  \n",
       "3  https://kworb.net/spotify/track/6sGIMrtIzQjdzN...  \n",
       "4  https://kworb.net/spotify/track/3yWuTOYDztXjZx...  \n",
       "5  https://kworb.net/spotify/track/2RkZ5LkEzeHGRs...  \n",
       "6  https://kworb.net/spotify/track/0FTmksd2dxiE5e...  \n",
       "7  https://kworb.net/spotify/track/2QjOHCTQ1Jl3za...  \n",
       "8  https://kworb.net/spotify/track/6Qyc6fS4DsZjB2...  \n",
       "9  https://kworb.net/spotify/track/2EjXfH91m7f8Hi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_us_daily_tracks():\n",
    "    r = requests.get(KWORB_DAILY_URL)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\")\n",
    "    rows = table.find_all(\"tr\")[1:]  # skip header\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "\n",
    "        if len(cols) < 7:\n",
    "            continue\n",
    "\n",
    "        rank = cols[0].text.strip()\n",
    "\n",
    "        # The \"Artist and Title\" column e.g. \"Taylor Swift - The Fate of Ophelia\"\n",
    "        artist_title = cols[2].text.strip()\n",
    "\n",
    "        # split on \" - \"\n",
    "        if \" - \" in artist_title:\n",
    "            artist_name, track_name = artist_title.split(\" - \", 1)\n",
    "        else:\n",
    "            artist_name = artist_title\n",
    "            track_name = \"\"\n",
    "\n",
    "        # raw links\n",
    "        _, track_url = extract_links(cols[2])\n",
    "\n",
    "        # true artist_songs_url\n",
    "        artist_albums_url = get_artist_songs_url_from_track(track_url)\n",
    "        \n",
    "        # Strer\n",
    "        daily_streams = cols[6].text.strip()\n",
    "        streams_plus = cols[7].text.strip() if len(cols) > 7 else None\n",
    "        seven_day = cols[8].text.strip() if len(cols) > 8 else None\n",
    "        seven_day_plus = cols[9].text.strip() if len(cols) > 9 else None\n",
    "        total_streams = cols[10].text.strip()\n",
    "\n",
    "        data.append({\n",
    "            \"rank\": rank,\n",
    "            \"artist_name\": artist_name,\n",
    "            \"track_name\": track_name,\n",
    "            \"daily_streams\": daily_streams,\n",
    "            \"streams_plus\": streams_plus,\n",
    "            \"seven_day\": seven_day,\n",
    "            \"seven_day_plus\": seven_day_plus,\n",
    "            \"total_streams\": total_streams,\n",
    "            \"artist_url\": artist_albums_url,\n",
    "            \"track_url\": track_url\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_chart = get_us_daily_tracks()\n",
    "df_chart.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f774b37d-be53-45b5-aa70-ceed119f78ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ JSON saved: outputs_kworb/spotify_us_daily_artists_20251120.json\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "df_chart.to_csv(f\"outputs_kworb/spotify_us_daily_artist_{today}.csv\", index=False)\n",
    "\n",
    "# ==== SAVE JSON ====\n",
    "json_path = f\"outputs_kworb/spotify_us_daily_artists_{today}.json\"\n",
    "df_chart.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
    "\n",
    "print(\"üéâ JSON saved:\", json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62608a9d-7b62-48d2-ab27-e56bb91ca00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b61fc5e-b433-4e12-a755-63513d717729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: outputs_kworb/kworb_us_daily_artist_20251119.csv\n",
      "Total artists: 200\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Load your existing Kworb daily artist file ===\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "input_path = f\"outputs_kworb/kworb_us_daily_artist_{today}.csv\"\n",
    "df_chart = pd.read_csv(input_path)\n",
    "\n",
    "print(\"Loaded file:\", input_path)\n",
    "print(\"Total artists:\", len(df_chart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0036478a-5c22-4cdb-94c8-c738e6b21cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://kworb.net\"\n",
    "\n",
    "def normalize_url(href):\n",
    "    if not href:\n",
    "        return None\n",
    "\n",
    "    url = urljoin(BASE + \"/\", href)\n",
    "\n",
    "    # Fix missing /spotify/\n",
    "    if \"/track/\" in url and \"/spotify/\" not in url:\n",
    "        url = url.replace(\"/track/\", \"/spotify/track/\")\n",
    "    if \"/artist/\" in url and \"/spotify/\" not in url:\n",
    "        url = url.replace(\"/artist/\", \"/spotify/artist/\")\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66531d7-6057-4608-84f9-922ee436c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_artist_id(artist_url):\n",
    "    \"\"\"\n",
    "    Extract Spotify artist_id from URL:\n",
    "    https://kworb.net/spotify/artist/<artist_id>_songs.html\n",
    "    \"\"\"\n",
    "    if not artist_url:\n",
    "        return None\n",
    "    filename = artist_url.split(\"/\")[-1]\n",
    "    artist_id = filename.replace(\"_songs.html\", \"\").strip()\n",
    "    return artist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd1b0d2-59e1-4de2-9057-354fbbcd6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_artist_top_songs(artist_name, artist_id, artist_url):\n",
    "    print(f\"   ‚Üí Scraping Top Songs for {artist_name}\")\n",
    "\n",
    "    url = artist_url\n",
    "    r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    if r.status_code != 200:\n",
    "        print(f\"     ‚ö†Ô∏è Failed: HTTP {r.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Kworb ÁöÑ Top Songs ÊòØÈ°µÈù¢‰∏äÁöÑÁ¨¨2‰∏™ table\n",
    "    tables = soup.find_all(\"table\")\n",
    "    if len(tables) < 2:\n",
    "        print(\"     ‚ö†Ô∏è ERROR: No Top Songs table found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    top_song_table = tables[2]   # Á¨¨ 2 ‰∏™Ë°®Ê†º\n",
    "\n",
    "    rows = top_song_table.find_all(\"tr\")[1:]  # Ë∑≥Ëøá header\n",
    "\n",
    "    data = []\n",
    "    for row in rows[:10]:  # Top 10 Songs\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "\n",
    "        rank = cols[0].text.strip()\n",
    "        song_title = cols[1].text.strip()\n",
    "        streams_total = cols[2].text.strip()\n",
    "        daily_streams = cols[3].text.strip()\n",
    "\n",
    "        data.append({\n",
    "            \"artist_name\": artist_name,\n",
    "            \"artist_id\": artist_id,\n",
    "            \"rank\": rank,\n",
    "            \"song_title\": song_title,\n",
    "            \"streams_total\": streams_total,\n",
    "            \"daily_streams\": daily_streams,\n",
    "            \"artist_url\": artist_url\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c500d8-791f-46dd-8577-45bb009a6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_artists_top10(df_chart):\n",
    "    all_data = []\n",
    "\n",
    "    artists = df_chart.dropna(subset=[\"artist_url\"]).drop_duplicates(\"artist_url\")\n",
    "\n",
    "    print(\"Total unique artists:\", len(artists))\n",
    "\n",
    "    for _, row in artists.iterrows():\n",
    "        artist_name = row[\"artist_name\"]\n",
    "        artist_url = normalize_url(row[\"artist_url\"])\n",
    "        artist_id = extract_artist_id(artist_url)\n",
    "\n",
    "        try:\n",
    "            df_artist = scrape_artist_top_songs(artist_name, artist_id, artist_url)\n",
    "            if len(df_artist) > 0:\n",
    "                all_data.append(df_artist)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error scraping {artist_name}: {e}\")\n",
    "\n",
    "    if len(all_data) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3119f938-ea0f-481c-8255-c0e5dc7c93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique artists: 115\n",
      "   ‚Üí Scraping Top Songs for Taylor Swift\n",
      "‚ö†Ô∏è Error scraping Taylor Swift: list index out of range\n",
      "   ‚Üí Scraping Top Songs for HUNTR/X\n",
      "‚ö†Ô∏è Error scraping HUNTR/X: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Olivia Dean\n",
      "‚ö†Ô∏è Error scraping Olivia Dean: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Alex Warren\n",
      "‚ö†Ô∏è Error scraping Alex Warren: list index out of range\n",
      "   ‚Üí Scraping Top Songs for sombr\n",
      "‚ö†Ô∏è Error scraping sombr: list index out of range\n",
      "   ‚Üí Scraping Top Songs for The Neighbourhood\n",
      "‚ö†Ô∏è Error scraping The Neighbourhood: list index out of range\n",
      "   ‚Üí Scraping Top Songs for The Goo Goo Dolls\n",
      "‚ö†Ô∏è Error scraping The Goo Goo Dolls: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Brenda Lee\n",
      "‚ö†Ô∏è Error scraping Brenda Lee: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Radiohead\n",
      "‚ö†Ô∏è Error scraping Radiohead: list index out of range\n",
      "   ‚Üí Scraping Top Songs for The Mar√É¬≠as\n",
      "‚ö†Ô∏è Error scraping The Mar√É¬≠as: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Mariah Carey\n",
      "‚ö†Ô∏è Error scraping Mariah Carey: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Tame Impala\n",
      "‚ö†Ô∏è Error scraping Tame Impala: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Wham!\n",
      "‚ö†Ô∏è Error scraping Wham!: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Billie Eilish\n",
      "‚ö†Ô∏è Error scraping Billie Eilish: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Fleetwood Mac\n",
      "‚ö†Ô∏è Error scraping Fleetwood Mac: list index out of range\n",
      "   ‚Üí Scraping Top Songs for Morgan Wallen\n",
      "‚ö†Ô∏è Error scraping Morgan Wallen: list index out of range\n",
      "   ‚Üí Scraping Top Songs for NF\n",
      "‚ö†Ô∏è Error scraping NF: list index out of range\n",
      "   ‚Üí Scraping Top Songs for The Killers\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_top10 = scrape_all_artists_top10(df_chart)\n\u001b[32m      3\u001b[39m output_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutputs_kworb/artist_top10_songs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m df_top10.to_csv(output_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mscrape_all_artists_top10\u001b[39m\u001b[34m(df_chart)\u001b[39m\n\u001b[32m     11\u001b[39m artist_id = extract_artist_id(artist_url)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     df_artist = scrape_artist_top_songs(artist_name, artist_id, artist_url)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_artist) > \u001b[32m0\u001b[39m:\n\u001b[32m     16\u001b[39m         all_data.append(df_artist)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mscrape_artist_top_songs\u001b[39m\u001b[34m(artist_name, artist_id, artist_url)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚Üí Scraping Top Songs for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00martist_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m url = artist_url\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m r = requests.get(url, headers={\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mMozilla/5.0\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m     ‚ö†Ô∏è Failed: HTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[33m\"\u001b[39m\u001b[33mget\u001b[39m\u001b[33m\"\u001b[39m, url, params=params, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m session.request(method=method, url=url, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28mself\u001b[39m.send(prep, **send_kwargs)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = adapter.send(request, **kwargs)\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = conn.urlopen(\n\u001b[32m    668\u001b[39m         method=request.method,\n\u001b[32m    669\u001b[39m         url=url,\n\u001b[32m    670\u001b[39m         body=request.body,\n\u001b[32m    671\u001b[39m         headers=request.headers,\n\u001b[32m    672\u001b[39m         redirect=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    673\u001b[39m         assert_same_host=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    674\u001b[39m         preload_content=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    675\u001b[39m         decode_content=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    676\u001b[39m         retries=\u001b[38;5;28mself\u001b[39m.max_retries,\n\u001b[32m    677\u001b[39m         timeout=timeout,\n\u001b[32m    678\u001b[39m         chunked=chunked,\n\u001b[32m    679\u001b[39m     )\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28mself\u001b[39m._make_request(\n\u001b[32m    788\u001b[39m     conn,\n\u001b[32m    789\u001b[39m     method,\n\u001b[32m    790\u001b[39m     url,\n\u001b[32m    791\u001b[39m     timeout=timeout_obj,\n\u001b[32m    792\u001b[39m     body=body,\n\u001b[32m    793\u001b[39m     headers=headers,\n\u001b[32m    794\u001b[39m     chunked=chunked,\n\u001b[32m    795\u001b[39m     retries=retries,\n\u001b[32m    796\u001b[39m     response_conn=response_conn,\n\u001b[32m    797\u001b[39m     preload_content=preload_content,\n\u001b[32m    798\u001b[39m     decode_content=decode_content,\n\u001b[32m    799\u001b[39m     **response_kw,\n\u001b[32m    800\u001b[39m )\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28mself\u001b[39m._validate_conn(conn)\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     conn.connect()\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/urllib3/connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    788\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n\u001b[32m    791\u001b[39m         sock=sock,\n\u001b[32m    792\u001b[39m         cert_reqs=\u001b[38;5;28mself\u001b[39m.cert_reqs,\n\u001b[32m    793\u001b[39m         ssl_version=\u001b[38;5;28mself\u001b[39m.ssl_version,\n\u001b[32m    794\u001b[39m         ssl_minimum_version=\u001b[38;5;28mself\u001b[39m.ssl_minimum_version,\n\u001b[32m    795\u001b[39m         ssl_maximum_version=\u001b[38;5;28mself\u001b[39m.ssl_maximum_version,\n\u001b[32m    796\u001b[39m         ca_certs=\u001b[38;5;28mself\u001b[39m.ca_certs,\n\u001b[32m    797\u001b[39m         ca_cert_dir=\u001b[38;5;28mself\u001b[39m.ca_cert_dir,\n\u001b[32m    798\u001b[39m         ca_cert_data=\u001b[38;5;28mself\u001b[39m.ca_cert_data,\n\u001b[32m    799\u001b[39m         cert_file=\u001b[38;5;28mself\u001b[39m.cert_file,\n\u001b[32m    800\u001b[39m         key_file=\u001b[38;5;28mself\u001b[39m.key_file,\n\u001b[32m    801\u001b[39m         key_password=\u001b[38;5;28mself\u001b[39m.key_password,\n\u001b[32m    802\u001b[39m         server_hostname=server_hostname_rm_dot,\n\u001b[32m    803\u001b[39m         ssl_context=\u001b[38;5;28mself\u001b[39m.ssl_context,\n\u001b[32m    804\u001b[39m         tls_in_tls=tls_in_tls,\n\u001b[32m    805\u001b[39m         assert_hostname=\u001b[38;5;28mself\u001b[39m.assert_hostname,\n\u001b[32m    806\u001b[39m         assert_fingerprint=\u001b[38;5;28mself\u001b[39m.assert_fingerprint,\n\u001b[32m    807\u001b[39m     )\n\u001b[32m    808\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/urllib3/connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = ssl_wrap_socket(\n\u001b[32m    970\u001b[39m     sock=sock,\n\u001b[32m    971\u001b[39m     keyfile=key_file,\n\u001b[32m    972\u001b[39m     certfile=cert_file,\n\u001b[32m    973\u001b[39m     key_password=key_password,\n\u001b[32m    974\u001b[39m     ca_certs=ca_certs,\n\u001b[32m    975\u001b[39m     ca_cert_dir=ca_cert_dir,\n\u001b[32m    976\u001b[39m     ca_cert_data=ca_cert_data,\n\u001b[32m    977\u001b[39m     server_hostname=server_hostname,\n\u001b[32m    978\u001b[39m     ssl_context=context,\n\u001b[32m    979\u001b[39m     tls_in_tls=tls_in_tls,\n\u001b[32m    980\u001b[39m )\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    983\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/urllib3/util/ssl_.py:480\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    476\u001b[39m         context.load_cert_chain(certfile, keyfile, key_password)\n\u001b[32m    478\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/site-packages/urllib3/util/ssl_.py:524\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    521\u001b[39m     SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sslsocket_class._create(\n\u001b[32m    456\u001b[39m         sock=sock,\n\u001b[32m    457\u001b[39m         server_side=server_side,\n\u001b[32m    458\u001b[39m         do_handshake_on_connect=do_handshake_on_connect,\n\u001b[32m    459\u001b[39m         suppress_ragged_eofs=suppress_ragged_eofs,\n\u001b[32m    460\u001b[39m         server_hostname=server_hostname,\n\u001b[32m    461\u001b[39m         context=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    462\u001b[39m         session=session\n\u001b[32m    463\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1073\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1074\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28mself\u001b[39m.do_handshake()\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/AML/lib/python3.13/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28mself\u001b[39m._sslobj.do_handshake()\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df_top10 = scrape_all_artists_top10(df_chart)\n",
    "\n",
    "output_path = f\"outputs_kworb/artist_top10_songs_{today}.csv\"\n",
    "df_top10.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"üéâ DONE! File saved:\", output_path)\n",
    "print(\"Total rows:\", len(df_top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130bc6e0-8679-4ab7-9d2f-a696ac4866ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables found: 2\n",
      "\n",
      "===== first 500 chars =====\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\"><head><title>Taylor Swift - Spotify Top Songs</title>\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/standard0002.css\" />\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/tables0010.css\" />\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/menus0001.css\" />\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
      "<meta name=\"description\" content=\"A website that collects and analyzes music data from around the world. All of the charts, sales and stre\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://kworb.net/spotify/artist/06HL4z0CvFAxyc27GXpf02_songs.html\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "tables = soup.find_all(\"table\")\n",
    "print(\"Tables found:\", len(tables))\n",
    "\n",
    "print(\"\\n===== first 500 chars =====\")\n",
    "print(r.text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb2eea-cd0f-458b-a2e0-ee3bad597962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc263d6-d8a9-444d-8828-c13efa1c1bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (4.38.0)\n",
      "Requirement already satisfied: webdriver-manager in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from selenium) (2025.11.12)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.1)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from webdriver-manager) (2.32.4)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from webdriver-manager) (1.2.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/AML/lib/python3.13/site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba8998c-772c-4755-b5b5-ea33e347decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def fetch_html_selenium(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # allow JS rendering\n",
    "\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "929c7c7e-d659-4c9a-9806-f97a9b0bbc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables found: 2\n"
     ]
    }
   ],
   "source": [
    "url = \"https://kworb.net/spotify/artist/06HL4z0CvFAxyc27GXpf02_songs.html\"\n",
    "html = fetch_html_selenium(url)\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "print(\"Tables found:\", len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f36593-1c7e-4456-98cb-e036b1306730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_artist_top_songs_selenium(artist_name, artist_id, artist_url):\n",
    "    print(f\"Scraping Top Songs for {artist_name} ...\")\n",
    "\n",
    "    html = fetch_html_selenium(artist_url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tables = soup.find_all(\"table\")\n",
    "\n",
    "    if len(tables) < 3:\n",
    "        print(\"‚ö†Ô∏è Top Songs table missing!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    top_table = tables[2]\n",
    "    rows = top_table.find_all(\"tr\")[1:]\n",
    "\n",
    "    songs = []\n",
    "    for row in rows[:10]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 4:\n",
    "            continue\n",
    "\n",
    "        songs.append({\n",
    "            \"artist_name\": artist_name,\n",
    "            \"artist_id\": artist_id,\n",
    "            \"rank\": cols[0].text.strip(),\n",
    "            \"song_title\": cols[1].text.strip(),\n",
    "            \"streams_total\": cols[2].text.strip(),\n",
    "            \"daily_streams\": cols[3].text.strip(),\n",
    "            \"artist_url\": artist_url\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66ca71be-39e8-4b9e-8914-be87fa1e8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://kworb.net/spotify/artist/06HL4z0CvFAxyc27GXpf02_songs.html\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "    \"Cookie\": \"DSID=AEhM4Mc962PI6BW1oNIw0j0cJXb0OYMDAUmU3XDRL4V7Pwmn1S40p2QByuGRiP2QSpPVEug_3RqsqXnNYZzW0DcbD-OPiKm6Uco8cmPpISSBY2KaqMk9r1ozwl16wDSVUEmR7ofD_4uF5vKOph-Y72R9R68HaBPQYRE-WpQUYwlROEjdZvpgLqpMa3f2X0iiP5pvPKuBmm9ieF0FcFVriQBWmruIPb-lskp0mEMF6Rq4gi9Rbcz3I1XmGCO1TNzFw74aQKNB4dvBmK-CLFk0fQEoyFWXOV4x-ZjivrdTapuiIIpGlNjA858; ar_debug=1; IDE=AHWqTUmvIq06TXInFKPSC59elllUkCbFx00g4qGZeN8nao0SP0UF7ec58PQSD_mT4_I; APC=AfxxVi5DECpIQVYAsPqu7dPM-yGt3fS3XQKATpM6eGG8w1s-PFib6A\",   # ÂøÖÈ°ª\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa9cecd3-4722-4060-8192-8d33055b7a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables found: 2\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "print(\"Tables found:\", len(soup.find_all(\"table\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da787756-015b-4795-9f32-e6ce9d218560",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Top 10 Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee0ddd-ffa8-497d-984c-95f57d63d09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595cf11-0fe5-4502-b3fd-699bb22b23c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0021a16-cd9e-477e-938a-3aba0e7d910a",
   "metadata": {},
   "source": [
    "### Spotify Top 10 Albums per each artist in US daily chart (200 artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40ed09c-dfff-4589-95db-051606cc25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4fc4bc0-4337-446c-b37d-262f826d3be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: outputs_kworb/spotify_us_daily_artist_20251120.csv\n",
      "Total artists: 200\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Load your existing Kworb daily artist file ===\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "input_path = f\"outputs_kworb/spotify_us_daily_artist_{today}.csv\"\n",
    "df_chart = pd.read_csv(input_path)\n",
    "\n",
    "print(\"Loaded file:\", input_path)\n",
    "print(\"Total artists:\", len(df_chart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b676d1-b895-43f2-b158-e852752492bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://kworb.net\"\n",
    "\n",
    "def normalize_url(href):\n",
    "    if not href:\n",
    "        return None\n",
    "\n",
    "    url = urljoin(BASE + \"/\", href)\n",
    "\n",
    "    # Fix missing /spotify/\n",
    "    if \"/track/\" in url and \"/spotify/\" not in url:\n",
    "        url = url.replace(\"/track/\", \"/spotify/track/\")\n",
    "    if \"/artist/\" in url and \"/spotify/\" not in url:\n",
    "        url = url.replace(\"/artist/\", \"/spotify/artist/\")\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75edc807-b955-462d-a65f-cba8d96bba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_artist_id(artist_url):\n",
    "    \"\"\"\n",
    "    Extract Spotify artist_id from URL:\n",
    "    https://kworb.net/spotify/artist/<artist_id>_albums.html\n",
    "    \"\"\"\n",
    "    if not artist_url:\n",
    "        return None\n",
    "    filename = artist_url.split(\"/\")[-1]\n",
    "    artist_id = filename.replace(\"_albums.html\", \"\").strip()\n",
    "    return artist_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f8ba32-6b49-4e8c-afa3-7e70ed884285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_artist_top10(artist_name, artist_id, artist_url):\n",
    "    try:\n",
    "        r = requests.get(artist_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        # Âè™‰ºöÊâæÂà∞‰∏ÄÂº† tableÔºåËøôÊòØÊ≠£Á°ÆÁöÑ\n",
    "        table = soup.find(\"table\")\n",
    "        if table is None:\n",
    "            print(f\"‚ö†Ô∏è No table for {artist_name}\")\n",
    "            return pd.DataFrame([])\n",
    "\n",
    "        rows = table.find_all(\"tr\")[1:]  # skip header\n",
    "\n",
    "        results = []\n",
    "        for tr in rows[:10]:\n",
    "            tds = tr.find_all(\"td\")\n",
    "\n",
    "            album_title = tds[0].text.strip()\n",
    "            streams = tds[1].text.strip()\n",
    "            daily = tds[2].text.strip()\n",
    "\n",
    "            results.append({\n",
    "                \"artist_name\": artist_name,\n",
    "                \"artist_id\": artist_id,\n",
    "                \"album_title\": album_title,\n",
    "                \"streams\": streams,\n",
    "                \"daily\": daily,\n",
    "                \"artist_url\": artist_url\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó Error scraping {artist_name}: {e}\")\n",
    "        return pd.DataFrame([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2cd5f7-fda7-4c0f-800f-cd809ae70367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_artists_top10(df_chart):\n",
    "    all_rows = []\n",
    "\n",
    "    # Ensure deduplication by artist_url\n",
    "    artists = df_chart.dropna(subset=[\"artist_url\"]).drop_duplicates(\"artist_url\")\n",
    "\n",
    "    print(\"Total unique artist URLs:\", len(artists))\n",
    "\n",
    "    for _, row in artists.iterrows():\n",
    "        artist_name = row[\"artist_name\"]\n",
    "        artist_url = normalize_url(row[\"artist_url\"])\n",
    "        artist_id = extract_artist_id(artist_url)\n",
    "\n",
    "        print(f\"üéµ Scraping Top 10 for {artist_name} ({artist_id})...\")\n",
    "\n",
    "        df_artist = scrape_artist_top10(artist_name, artist_id, artist_url)\n",
    "        all_rows.append(df_artist)\n",
    "\n",
    "    return pd.concat(all_rows, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70696e3f-cac8-4116-8fd9-df55702dc793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique artist URLs: 115\n",
      "üéµ Scraping Top 10 for Taylor Swift (06HL4z0CvFAxyc27GXpf02)...\n",
      "üéµ Scraping Top 10 for HUNTR/X (2yNNYQBChuox9A5Ka93BIn)...\n",
      "üéµ Scraping Top 10 for Olivia Dean (00x1fYSGhdqScXBRpSj3DW)...\n",
      "üéµ Scraping Top 10 for Alex Warren (0fTSzq9jAh4c36UVb4V7CB)...\n",
      "üéµ Scraping Top 10 for sombr (4G9NDjRyZFDlJKMRL8hx3S)...\n",
      "üéµ Scraping Top 10 for The Neighbourhood (77SW9BnxLY8rJ0RciFqkHh)...\n",
      "üéµ Scraping Top 10 for The Goo Goo Dolls (2sil8z5kiy4r76CRTXxBCA)...\n",
      "üéµ Scraping Top 10 for Brenda Lee (4cPHsZM98sKzmV26wlwD2W)...\n",
      "üéµ Scraping Top 10 for Radiohead (4Z8W4fKeB5YxbusRsdQVPb)...\n",
      "üéµ Scraping Top 10 for The Mar√É¬≠as (2sSGPbdZJkaSE2AbcGOACx)...\n",
      "üéµ Scraping Top 10 for Mariah Carey (4iHNK0tOyZPYnBU7nGAgpQ)...\n",
      "üéµ Scraping Top 10 for Tame Impala (5INjqkS1o8h1imAzPqGZBb)...\n",
      "üéµ Scraping Top 10 for Wham! (5lpH0xAS4fVfLkACg9DAuM)...\n",
      "üéµ Scraping Top 10 for Billie Eilish (6qqNVTkY8uBg9cP3Jd7DAH)...\n",
      "üéµ Scraping Top 10 for Fleetwood Mac (08GQAI4eElDnROBrJRGE0X)...\n",
      "üéµ Scraping Top 10 for Morgan Wallen (4oUHIQIBe0LHzYfvXNW4QM)...\n",
      "üéµ Scraping Top 10 for NF (6fOMl44jA4Sp5b9PpYCkzz)...\n",
      "üéµ Scraping Top 10 for The Killers (0C0XlULifJtAgn6ZNCW2eu)...\n",
      "üéµ Scraping Top 10 for Cody Johnson (6zLBxLdl60ekBLpawtT63I)...\n",
      "üéµ Scraping Top 10 for Coldplay (4gzpq5DPGxSnKTe4SA8HAU)...\n",
      "üéµ Scraping Top 10 for Saja Boys (0BJ2EjOqcfgFvlZXNKrNbE)...\n",
      "üéµ Scraping Top 10 for Bobby Helms (38EmEgXkgK51MT2tPY0EoC)...\n",
      "üéµ Scraping Top 10 for RAYE (5KKpBU5eC2tJDzf0wmlRp2)...\n",
      "üéµ Scraping Top 10 for Arctic Monkeys (7Ln80lUS6He07XvHI8qqHH)...\n",
      "üéµ Scraping Top 10 for Lady Gaga (1HY2Jd0NmPuamShAr6KMms)...\n",
      "üéµ Scraping Top 10 for Don Toliver (4Gso3d4CscCijv0lmajZWs)...\n",
      "üéµ Scraping Top 10 for Tate McRae (45dkTj5sMRSjrmBSBeiHym)...\n",
      "üéµ Scraping Top 10 for Teddy Swims (33qOK5uJ8AR2xuQQAhHump)...\n",
      "üéµ Scraping Top 10 for The Weeknd (1Xyo4u8uXC1ZmMpatF05PJ)...\n",
      "üéµ Scraping Top 10 for Ariana Grande (66CXWjxzNUsdJxJ2JdwvnR)...\n",
      "üéµ Scraping Top 10 for Drake (3TVXtAsR1Inumwj472S9r4)...\n",
      "üéµ Scraping Top 10 for Ravyn Lenae (5RTLRtXjbXI2lSXc6jxlAz)...\n",
      "üéµ Scraping Top 10 for Sabrina Carpenter (74KM79TiuVKeVCqs8QtB0B)...\n",
      "üéµ Scraping Top 10 for Disco Lines (5Kmr0b3ip8g9P2i0dLTC3Z)...\n",
      "üéµ Scraping Top 10 for Zach Bryan (40ZNYROS4zLfyyBSs2PGe2)...\n",
      "üéµ Scraping Top 10 for Ella Langley (6BRxQ8cD3eqnrVj6WKDok8)...\n",
      "üéµ Scraping Top 10 for Chappell Roan (7GlBOeep6PqTfFi59PTUUN)...\n",
      "üéµ Scraping Top 10 for KATSEYE (3c0gDdb9lhnHGFtP4prQpn)...\n",
      "üéµ Scraping Top 10 for Hazbin Hotel (3trytB1YUZ6SSOKlMcnK1l)...\n",
      "üéµ Scraping Top 10 for Kendrick Lamar (2YZyLoL8N0Wb9xBt1NhZWg)...\n",
      "üéµ Scraping Top 10 for Nat King Cole (7v4imS0moSyGdXyLgVTIV7)...\n",
      "üéµ Scraping Top 10 for Andy Williams (4sj6D0zlMOl25nprDJBiU9)...\n",
      "üéµ Scraping Top 10 for Justin Bieber (1uNFoZAHBGtllmzznpCI3s)...\n",
      "üéµ Scraping Top 10 for RUMI (5sUZGmvurxH1pOXNabE3yJ)...\n",
      "‚ö†Ô∏è No table for RUMI\n",
      "üéµ Scraping Top 10 for Gigi Perez (1iCnM8foFssWlPRLfAbIwo)...\n",
      "üéµ Scraping Top 10 for Riley Green (2QMsj4XJ7ne2hojxt6v5eb)...\n",
      "üéµ Scraping Top 10 for Benson Boone (22wbnEMDvgVIAGdFeek6ET)...\n",
      "üéµ Scraping Top 10 for Michael Bubl√É¬© (1GxkXlMwML1oSg5eLPiAz3)...\n",
      "üéµ Scraping Top 10 for Kelly Clarkson (3BmGtnKgCSGYIUhmivXKWX)...\n",
      "üéµ Scraping Top 10 for Kehlani (0cGUm45nv7Z6M6qdXYQGTX)...\n",
      "üéµ Scraping Top 10 for Chris Stapleton (4YLtscXsxbVgi031ovDDdh)...\n",
      "üéµ Scraping Top 10 for Gorillaz (3AA28KZvwAUcZuOKwyblJQ)...\n",
      "üéµ Scraping Top 10 for Dean Martin (49e4v89VmlDcFCMyDv9wQ9)...\n",
      "üéµ Scraping Top 10 for Childish Gambino (73sIBHcqh3Z3NyqHKZ7FOL)...\n",
      "üéµ Scraping Top 10 for YoungBoy Never Broke Again (7wlFDEWiM5OoIAt8RSli8b)...\n",
      "üéµ Scraping Top 10 for Tyler, The Creator (4V8LLVI7PbaPR0K2TGSxFF)...\n",
      "üéµ Scraping Top 10 for Cigarettes After Sex (1QAJqy2dA3ihHBFIHRphZj)...\n",
      "üéµ Scraping Top 10 for J. Cole (6l3HvQ5sa6mXTsMTB19rO5)...\n",
      "üéµ Scraping Top 10 for Laufey (7gW0r5CkdEUMm42w9XpyZO)...\n",
      "üéµ Scraping Top 10 for Post Malone (246dkjvS1zLTtiykXe5h60)...\n",
      "üéµ Scraping Top 10 for Nelly Furtado (2jw70GZXlAI8QzWeY2bgRc)...\n",
      "üéµ Scraping Top 10 for Bad Bunny (4q3ewBCX7sLwd24euuV69X)...\n",
      "üéµ Scraping Top 10 for Kanye West (5K4W6rqBFWDnAN6FQUkS6x)...\n",
      "üéµ Scraping Top 10 for The Police (5NGO30tJxFlKixkPSgXcFE)...\n",
      "üéµ Scraping Top 10 for Mazzy Star (37w38cCSGgKLdayTRjna4W)...\n",
      "üéµ Scraping Top 10 for Hudson Westbrook (0C4gtx1iHMfuaQ73GKWvtZ)...\n",
      "üéµ Scraping Top 10 for Whiskey Myers (26opZSJcXshCmCwxgZQmBc)...\n",
      "üéµ Scraping Top 10 for Joji (3MZsBdqDrRTJihTHQrO6Dq)...\n",
      "üéµ Scraping Top 10 for Treaty Oak Revival (3444S3C4U9Ts86BnCtSPRV)...\n",
      "üéµ Scraping Top 10 for Waka Flocka Flame (6f4XkbvYlXMH0QgVRzW0sM)...\n",
      "üéµ Scraping Top 10 for Miguel (360IAlyVv4PCEVjgyMZrxK)...\n",
      "üéµ Scraping Top 10 for Daniel Caesar (20wkVLutqVOYrc0kxFs7rA)...\n",
      "üéµ Scraping Top 10 for TV Girl (0Y6dVaC9DZtPNH4591M42W)...\n",
      "üéµ Scraping Top 10 for Foo Fighters (7jy3rLJdDQY21OgRLCZ9sD)...\n",
      "üéµ Scraping Top 10 for Yeat (3qiHUAX7zY4Qnjx8TNUzVx)...\n",
      "üéµ Scraping Top 10 for Darlene Love (391oLRVmoTkumiN79HkTWu)...\n",
      "üéµ Scraping Top 10 for Noah Kahan (2RQXRUsr4IW1f3mKyKsy4B)...\n",
      "üéµ Scraping Top 10 for The Red Clay Strays (6IKlXZEFOvk9itrP1s0knJ)...\n",
      "üéµ Scraping Top 10 for Shaboozey (3y2cIKLjiOlp1Np37WiUdH)...\n",
      "üéµ Scraping Top 10 for Nirvana (6olE6TJLqED3rqDCT0FyPh)...\n",
      "üéµ Scraping Top 10 for Fuerza Regida (0ys2OFYzWYB5hRDLCsBqxt)...\n",
      "üéµ Scraping Top 10 for The Kid LAROI (2tIP7SsRs7vjIcLrU85W8J)...\n",
      "üéµ Scraping Top 10 for Tory Lanez (2jku7tDXc6XoB6MO2hFuqg)...\n",
      "üéµ Scraping Top 10 for Rihanna (5pKCCKE2ajJHZ9KAiaK11H)...\n",
      "üéµ Scraping Top 10 for Lil Wayne (55Aa2cqylxrFIXC767Z865)...\n",
      "üéµ Scraping Top 10 for Bruno Mars (0du5cEVh5yTK9QJze8zA0C)...\n",
      "üéµ Scraping Top 10 for Gracie Abrams (4tuJ0bMpJh08umKkEXKUI5)...\n",
      "üéµ Scraping Top 10 for Leon Thomas (0nnBZ8FXWjG9wZgM2cpfeb)...\n",
      "üéµ Scraping Top 10 for Frank Ocean (2h93pZq0e7k5yf4dywlkpM)...\n",
      "üéµ Scraping Top 10 for BigXthaPlug (6qxpnaukVayrQn6ViNvu9I)...\n",
      "üéµ Scraping Top 10 for Evanescence (5nGIFgo0shDenQYSE0Sn7c)...\n",
      "üéµ Scraping Top 10 for Lord Huron (6ltzsmQQbmdoHHbLZ4ZN25)...\n",
      "üéµ Scraping Top 10 for The Ronettes (7CyeXFnOrfC1N6z4naIpgo)...\n",
      "üéµ Scraping Top 10 for A$AP Rocky (13ubrt8QOOCPljQ2FL1Kca)...\n",
      "üéµ Scraping Top 10 for Michael Jackson (3fMbdgg4jU18AjLCKBhRSm)...\n",
      "üéµ Scraping Top 10 for Djo (5p9HO3XC5P3BLxJs5Mtrhm)...\n",
      "üéµ Scraping Top 10 for ROSAL√É¬çA (7ltDVBr6mKbRvohxheJ9h1)...\n",
      "üéµ Scraping Top 10 for Vance Joy (10exVja0key0uqUkk6LJRT)...\n",
      "üéµ Scraping Top 10 for Sia (5WUlDfRSoLAfcVSX1WnrxN)...\n",
      "üéµ Scraping Top 10 for Junior H (7Gi6gjaWy3DxyilpF1a8Is)...\n",
      "üéµ Scraping Top 10 for Red Hot Chili Peppers (0L8ExT028jH3ddEcZwqJJ5)...\n",
      "üéµ Scraping Top 10 for Linkin Park (6XyY86QOPPrYVGvF9ch6wz)...\n",
      "üéµ Scraping Top 10 for Hotel Ugly (35WVTyRnKAoaGExqgktVyb)...\n",
      "üéµ Scraping Top 10 for Jeff Buckley (3nnQpaTvKb5jCQabZefACI)...\n",
      "üéµ Scraping Top 10 for SZA (7tYKF4w9nC0nq9CsPZTHyP)...\n",
      "üéµ Scraping Top 10 for Hozier (2FXC3k01G6Gw61bmprjgqS)...\n",
      "üéµ Scraping Top 10 for Lil Uzi Vert (4O15NlyKLIASxsJ0PrXPfz)...\n",
      "üéµ Scraping Top 10 for wifiskeleton (0TvvDswOgvPbgSPK2RDf62)...\n",
      "üéµ Scraping Top 10 for Journey (0rvjqX7ttXeg3mTy8Xscbt)...\n",
      "üéµ Scraping Top 10 for Bryson Tiller (2EMAnMvWE2eb56ToJVfCWs)...\n",
      "üéµ Scraping Top 10 for The Cranberries (7t0rwkOPGlDPEhaOcVtOt9)...\n",
      "üéµ Scraping Top 10 for Diplo (5fMUXHkw8R8eOP2RNVYEZX)...\n",
      "üéµ Scraping Top 10 for EsDeeKid (0EyhkwP3UnwGFBy6xwKjSy)...\n",
      "üéµ Scraping Top 10 for Tears For Fears (4bthk9UfsYUYdcFyqxmSUU)...\n",
      "üéµ Scraping Top 10 for Three Days Grace (2xiIXseIJcq3nG7C8fHeBj)...\n",
      "üéâ DONE! File saved: outputs_kworb/spotify_artist_top10_ablums_20251120.csv\n",
      "Total rows: 966\n"
     ]
    }
   ],
   "source": [
    "df_top10 = scrape_all_artists_top10(df_chart)\n",
    "\n",
    "output_path = f\"outputs_kworb/spotify_artist_top10_ablums_{today}.csv\"\n",
    "df_top10.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"üéâ DONE! File saved:\", output_path)\n",
    "print(\"Total rows:\", len(df_top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feb7a059-d684-43a4-b099-30f571141416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ JSON saved: outputs_kworb/spotify_artist_top10_albums_20251120.json\n"
     ]
    }
   ],
   "source": [
    "# ==== SAVE JSON ====\n",
    "json_path = f\"outputs_kworb/spotify_artist_top10_albums_{today}.json\"\n",
    "df_top10.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
    "\n",
    "print(\"üéâ JSON saved:\", json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae728b-5677-4f6c-a20a-20f9f822df73",
   "metadata": {},
   "source": [
    "### Spotify top artists by monthly listeners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdaacd2-e3ab-472b-815a-403a1ee22d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ DONE! File saved: outputs_kworb/spotify_monthly_listeners_20251120.csv\n",
      "Total rows: 2500\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "URL = \"https://kworb.net/spotify/listeners.html\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def scrape_monthly_listeners():\n",
    "    r = requests.get(URL, headers=HEADERS)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\")\n",
    "    if table is None:\n",
    "        raise ValueError(\"‚ùó No table found on listeners page!\")\n",
    "\n",
    "    rows = table.find_all(\"tr\")[1:]  # skip header\n",
    "\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if len(tds) < 6:\n",
    "            continue\n",
    "\n",
    "        rank = tds[0].text.strip()\n",
    "\n",
    "        # Artist name is inside <a> tag\n",
    "        artist_name = tds[1].text.strip()\n",
    "        artist_url = tds[1].find(\"a\")[\"href\"] if tds[1].find(\"a\") else None\n",
    "\n",
    "        listeners = tds[2].text.strip()\n",
    "        daily_change = tds[3].text.strip()\n",
    "        peak_rank = tds[4].text.strip()\n",
    "        peak_listeners = tds[5].text.strip()\n",
    "\n",
    "        data.append({\n",
    "            \"rank\": rank,\n",
    "            \"artist_name\": artist_name,\n",
    "            \"artist_url\": \"https://kworb.net/spotify/\" + artist_url if artist_url else None,\n",
    "            \"listeners\": listeners,\n",
    "            \"daily_change\": daily_change,\n",
    "            \"peak_rank\": peak_rank,\n",
    "            \"peak_listeners\": peak_listeners\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# ==== RUN & SAVE ====\n",
    "df = scrape_monthly_listeners()\n",
    "\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "output_path = f\"outputs_kworb/spotify_monthly_listeners_{today}.csv\"\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"üéâ DONE! File saved:\", output_path)\n",
    "print(\"Total rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f834d2bb-0f5f-477f-9ade-147aada20d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ JSON saved: outputs_kworb/spotify_monthly_listeners_20251120.json\n"
     ]
    }
   ],
   "source": [
    "# ==== SAVE JSON ====\n",
    "json_path = f\"outputs_kworb/spotify_monthly_listeners_{today}.json\"\n",
    "df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
    "\n",
    "print(\"üéâ JSON saved:\", json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae54102-0d69-41cc-ba00-06ca42da2c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
